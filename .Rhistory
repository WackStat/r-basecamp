quiet.library("SparkR")
sc <- sparkR.init(master = "local"
,appName = "SparkR_Interactive"
,sparkPackages="com.databricks:spark-csv_2.10:1.2.0"
)
sqlContext <- sparkRSQL.init(sc)
sampleDF <- createDataFrame(sqlContext, faithful)
quiet.library("SparkR")
sc <- sparkR.init(master = "local"
,appName = "SparkR_Interactive"
,sparkPackages="com.databricks:spark-csv_2.10:1.2.0"
,sparkJars="spark-csv-assembly-1.1.0.jar"
)
sqlContext <- sparkRSQL.init(sc)
sampleDF <- createDataFrame(sqlContext, faithful)
quiet.library("SparkR")
sc <- sparkR.init(# define the master and the name of the application
master = "local"
,appName = "SparkR_Interactive"
# add spark package and dependencies
,sparkPackages="com.databricks:spark-csv_2.10:1.2.0"
,sparkJars="spark-csv-assembly-1.1.0.jar"
)
flights <- read.df(sqlContext
,path = "D:\\Dev\\datasets\\nycflights13.csv"
,"com.databricks.spark.csv"
,header = "true")
source('D:/Dev/r-basecamp/r-sparkR-quick-sheet.r', echo=TRUE)
quiet.library("SparkR")
sc <- sparkR.init(# define the master and the name of the application
master = "local"
,appName = "SparkR_Interactive"
# add spark package and dependencies
,sparkPackages="com.databricks:spark-csv_2.10:1.2.0"
,sparkJars="spark-csv-assembly-1.1.0.jar"
)
sqlContext <- sparkRSQL.init(sc)
flights <- read.df(sqlContext
,path = "D:\\Dev\\datasets\\nycflights13.csv"
,"com.databricks.spark.csv"
,header = "true")
flights <- read.df(sqlContext
,path = "D:\\Dev\\ds-fun-datasets\\nycflights2013.csv"
,"com.databricks.spark.csv"
,header = "true")
head(flights)
quiet.library("SparkR")
sc <- sparkR.init(# define the master and the name of the application
master = "local"
,appName = "SparkR_Interactive"
# add spark package and dependencies
,sparkPackages="com.databricks:spark-csv_2.11:1.3.0"
#                 ,sparkJars="spark-csv-assembly-1.1.0.jar"
)
sqlContext <- sparkRSQL.init(sc)
sampleDF <- createDataFrame(sqlContext, faithful)
quiet.library("SparkR")
sc <- sparkR.init(# define the master and the name of the application
master = "local"
,appName = "SparkR_Interactive"
# add spark package and dependencies
,sparkPackages="com.databricks:spark-csv_2.10:1.3.0"
#                 ,sparkJars="spark-csv-assembly-1.1.0.jar"
)
# initalize a new SQLContext so we can work with DataFrames
sqlContext <- sparkRSQL.init(sc)
sampleDF <- createDataFrame(sqlContext, faithful)
help.start()   # general help
apropos("foo") # list all functions containing string foo
help("foo")    # help about function foo
?foo           # same thing, help about function foo
example("foo") # show an example of function foo
RSiteSearch("foo")
vignette()      # show available vingettes
vignette("foo") # show specific vignette
getwd()         # print the current working directory
setwd("myPath")   # set the working directory to myPath string
pathBackslash = "c:\\myDirectory\\myFile.txt"
pathSlash = "c:/myDirectory/myFile.txt"
options()           # view current option settings
options(digits=3)   # number of digits to print on output
getOption("option") # Retrieve the value of an option
getOption("digit")
getOption("digits")
ls()                       # list the objects in the current workspace
rm("objectA","objectB")    # remove the object from the workspace
history()                  # display last 25 command
installed.packages()                  # list all installed packaged
.libPaths()                           # get library location
library()                             # see all libraries installed
search()                              # see libraries currently loaded
a <- c(-1,0,1,2)
ls()
b <- c("a","b","c")
C <- c(TRUE, FALSE, TRUE)
a[2]
b[c(1,3)]
b[c]
d <- c(1..10)
d <- c(1...10)
d <- C(1:10)
d <- c(1:10)
d
d[-2]
d[c(-2)]
y<-matrix(1:20, nrow=5,ncol=4)
y
x<-matrix(1:4, nrow=5,ncol=4,byrow = TRUE)
x
x<-matrix(1:20, nrow=5,ncol=4,byrow = TRUE)
x
x<-matrix(1:5, nrow=5,ncol=4,byrow = TRUE)
x
yCells <- c(1,6,19,80)
yCNames <- c("C1","C2")
yRNames <- c("R1","R2")
X <- matrix(yCells,nrows=2,crows=2,byrow = TRUE)
rm("x")
y <- matrix(yCells,nrows=2,crows=2,byrow = TRUE)
yCells <- c(1,6,19,80)
yRNames <- c("R1","R2")
yCNames <- c("C1","C2")
y
y <- matrix(yCells,nrows=2,ncols=2,byrow = TRUE)
y <- matrix(yCells,nrow=2,ncol=2,byrow = TRUE)
yNames <- list(yRNames, yCNames)
X[1,]
x <- matrix(1:20, nrow=5,ncol=4)
X[1,]
x <- matrix(1:20, nrow=5,ncol=4)
x
x[1,]
x[3,]
x[,3]
x[1,3]
x[c(2,4),3]
x[c(1,2),2:4]
x
x[c(1,2),2:4]
x[,-4]
x[-1,-4]
help("Arrays")
help(Arrays)
??Arrays
apropos("Array")
help("Arrays")
help("Array")
help("array")
y["C1",]
y <- matrix(yCells,nrow=2,ncol=2,byrow = TRUE, dimnames = yNames)
y["C1",]
y[C1,]
y["C1",]
y
y["C1",]
help("matrix")
is.matrix(a)
is.matrix(y)
as.matrix(a)
is.vector(a)
as.vector("str")
help("array")
dim(y)
dim(d)
dim(c)
i <- array(iCells,iDim, dimnames = iNames)
iCells <- c(1:36)
iDim <- c(3,3,4)
iANames <- c("A1","A2","A3")
iBNames <- c("B1","B2","B3")
iCNames <- c("C1","C2","C3","C4")
i <- array(iCells,iDim, dimnames = iNames)
iNames <- list(iANames, iBNames, iCNames)
i <- array(iCells,iDim, dimnames = iNames)
i
i=[,,1]
i[,,1]
i["A1",c(1,3),]
i[,-"B1",]
i[,-2,]
is.array(i)
as.array(y)
dim(i)
dimname(i)
dimnames(i)
help("vector")
cnd <- factor(c(rep("cat",10),rep("dog",20)))
help("factor")
cnd <- factor(c(rep("yes",10),rep("no",20)))
cnd
cnd[[1]]
cnd[[2]
]
cnd[[11]]
help("rep")
rep(c(1,2,3),10)
cod <- factor(c(rep("yes",10),rep("no",20),"maybe"))
cod
c(1,c(2,3,4),5:10)
cod <- factor(c(rep("yes",10),rep("no",20),"maybe"), levels = c("yes","no"))
cod
cod <- factor(c(rep("y",10),rep("n",20),"maybe"),
levels = c("yes","no"),
labels = c("yes","no"))
cod
cod <- factor(c(rep("y",10),rep("n",20),"maybe"),
levels = c("y","n"),
labels = c("yes","no"))
cod
is.factor(cnd)
is.ordered(cod)
f3 <- factor(c(rep("y",10),rep("n",20),"maybe"),
ordered = TRUE)
is.ordered(f2)
f1 <- factor(c(rep("y",10),rep("n",20)))
f2 <- factor(c(rep("y",10),rep("n",20),"maybe"),
levels = c("y","n"),
labels = c("yes","no"))
f3 <- factor(c(rep("y",10),rep("n",20),"maybe"),
ordered = TRUE)
f1[[1]]
is.factor(f1)
is.ordered(f2)
is.ordered(f3)
is.factor(b)
b
as.factor(b)
f2[1]
f2
f2[11]
summary(a)
summary(y)
summary(i)
sample(0:1, 20, replace = TRUE)
sample(0:1, 20, replace = FALSE)
sample(0:1, 20, replace = TRUE)
levels(f1)
labels(f2)
labels(f1)
labels(f3)
length(object) # number of elements or components
str(object)    # structure of an object
class(object)  # class or type of an object
length(f3)
length(q)
length(a)
class(a)
str(a)
str(f1)
str(levels(f2))
class(levels(f3))
fix(a)
a
fix(x)
fix(i)
fix(a)
fix(x)
dimnames(i)
names(i)
names(a)
names(b)
names(f2)
cbind(1,2,3)
rbind(1:10)
rbind(c(1:10))
proc.time()
start <- proc.time()
proc.time()
proc.time() -start
ptm <- proc.time()
for (i in 1:50) mad(stats::runif(500))
proc.time() - ptm
slowadd <- function(g){
h <- rep(NA, length(g))
for (i in 1:length(g)){
h[i] <- g[i] + 1
}
return(h)
}
system.time(a <- slowadd(g))
system.time(a <- slowadd(0))
proc.time()
nrow(i)
iCells <- c(1:36)
iDim <- c(3,3,4)
iANames <- c("A1","A2","A3")
iBNames <- c("B1","B2","B3")
iCNames <- c("C1","C2","C3","C4")
iNames <- list(iANames, iBNames, iCNames)
i <- array(iCells,iDim, dimnames = iNames)
nrow(i)
ncol(i)
dimnames(y)
dim(y)
nrow(x)
ncol(x)
nrow(f1)
quiet.library("SparkR")
,sparkPackages="com.databricks:spark-csv_2.10:1.3.0"
sc <- sparkR.init(master = "local"
,appName = "SparkR_Interactive"
,sparkPackages="com.databricks:spark-csv_2.10:1.3.0"
#                 ,sparkJars="spark-csv-assembly-1.1.0.jar"
)
# initalize a new SQLContext so we can work with DataFrames
sqlContext <- sparkRSQL.init(sc)
flights <- read.df(sqlContext
,path = "D:\\Dev\\ds-fun-datasets\\nycflights2013.csv"
,"com.databricks.spark.csv"
,header = "true")
head(flights)
flights <- read.df(sqlContext
,path = "D:/Dev/ds-fun-datasets/nycflights2013.csv"
,"com.databricks.spark.csv"
,header = "true")
quiet.library("SparkR")
sc <- sparkR.init(master = "local"
,appName = "SparkR_Interactive"
,sparkPackages="com.databricks:spark-csv_2.10:1.3.0"
#                 ,sparkJars="spark-csv-assembly-1.1.0.jar"
)
